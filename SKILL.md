---
name: structured-extractor
description: "通用结构化信息提取专家。Use when: 需要从代码/文档/日志中提取
  业务规则、事件流、状态机、约束、实体、关系。适用于: 代码分析、文档结构化、
  日志分析、知识库构建、结构化提取。基于 Google LangExtract 算法层重构。
  提供: (1) 6种提取类型分类框架 (2) Few-shot模板库
  (3) 6步后处理管道 (Source Grounding/去重/评分/消歧/关系推断/KG注入)。
  无需外部 API。"
---

# Structured Extractor

> 通用结构化信息提取 | 基于 Google LangExtract 算法层重构 | 内置 6 步后处理管道

## 核心禁令 (CRITICAL)

1. **禁止跳过 Few-shot** - 提取时必须按 `references/few-shot-templates.md` 中的模板输出
2. **禁止自由格式输出** - 必须严格遵循 `references/output-schema.md` 定义的 JSON Schema
3. **禁止跳过后处理** - 原始提取必须经过 `scripts/pipeline.py` 处理后才算完成
4. **禁止低质量注入** - KG 注入时 confidence < 0.3 的提取必须过滤

## 快速决策树

```
我需要从文本中提取结构化信息?
    │
    ├─ 文本类型是什么?
    │   ├─ .cs/.py/.js/.ts → 使用 code-logic 预设
    │   ├─ .md/.txt/.doc → 使用 doc-structure 预设
    │   ├─ .log → 使用 log-analysis 预设
    │   └─ 其他/自定义 → 手动指定 focus_types
    │
    ├─ 需要哪些后处理?
    │   ├─ 基础 (默认) → Grounding + Dedup + Scoring
    │   ├─ 完整 → 全部 6 步
    │   └─ 最小 → 仅 Scoring
    │
    └─ 结果用途?
        ├─ 生成报告 → 输出 Markdown
        ├─ 补充文档 → 输出到 docs/module-document/
        └─ 更新知识库 → 启用 KG Injection
```

## 工作流

### Step 1: 场景识别

根据文件类型自动选择预设方案:

```
预设文件: assets/presets/{preset-name}.json
├── code-logic.json       重点: rule, event, state, constraint
├── doc-structure.json    重点: entity, relation, rule, constraint
└── log-analysis.json     重点: event, state, constraint
```

### Step 2: Claude 提取

按以下规则从文本中提取结构化信息:

**6 种提取类型** (详见 `references/extraction-types.md`):

| 类型 | 识别特征 |
|------|---------|
| `rule` | 条件分支、业务决策 (if/switch/when) |
| `event` | 事件订阅、消息发送 (+=, Fire, Send) |
| `state` | 状态变化、阶段转换 (SetState, enum change) |
| `constraint` | null检查、边界条件、前置断言 |
| `entity` | 类定义、关键概念、角色 |
| `relation` | 实体间关系 (继承、依赖、调用) |

**输出格式** (每个提取项):

```json
{
  "id": "ext_001",
  "type": "rule",
  "text": "if (MLevel.IsGuideLevel)",
  "summary_cn": "新手引导关卡特殊处理",
  "attributes": {
    "condition": "IsGuideLevel == true",
    "action": "自动上阵并进入战斗"
  },
  "source_file": "AMultiGateLevel.cs"
}
```

**Few-shot 模板**: 见 `references/few-shot-templates.md`

### Step 3: 保存原始提取

将 Claude 提取的 JSON 保存到临时文件:

```bash
# 保存为 /tmp/extractions_raw.json 或指定位置
```

### Step 4: 运行后处理管道

```bash
# 基础用法
python scripts/pipeline.py --input raw.json --source code.cs --output result.json

# 使用预设配置
python scripts/pipeline.py --input raw.json --source code.cs --config assets/presets/code-logic.json --output result.json

# 启用全部功能
python scripts/pipeline.py --input raw.json --source code.cs --enable-entity-resolution --enable-relation-inference --enable-kg-injection --output result.json
```

**管道步骤** (详见 `references/post-processing.md`):

```
1. Source Grounding  → 精确定位 (char_start/end + line)
2. Overlap Dedup     → 去除重复提取
3. Confidence Score  → 4维度质量评分
4. Entity Resolution → 同义实体合并 (可选)
5. Relation Inference → 共现关系推断 (可选)
6. KG Injection      → 知识图谱注入 (可选)
```

### Step 5: 输出结果

读取 `extractions_final.json`，按需生成:
- Markdown 报告
- 知识图谱实体 (调用 aim_create_entities)
- 补充到模块文档

## 预设配置快速参考

| 预设 | 重点类型 | 默认后处理 |
|------|---------|-----------|
| `code-logic` | rule, event, state, constraint | Grounding + Dedup + Score |
| `doc-structure` | entity, relation, rule, constraint | 全部 6 步 |
| `log-analysis` | event, state, constraint | Grounding + Dedup + Score |

## 核心原则

1. **Few-shot 驱动** - 提取质量的关键在于 Few-shot 模板，不是自由发挥
2. **Pipeline 后处理** - 原始提取必须经过 Python 管道处理才算完成
3. **text 必须来自原文** - 提取的 text 字段是原文精确片段，不是改写或翻译

---

## Red Flags

看到这些信号时立即检查：

| 红旗信号 | 正确做法 |
|----------|----------|
| "直接输出 Markdown 报告就行" | 必须先走 JSON 提取 + pipeline 后处理 |
| "不需要 Few-shot，我知道怎么提取" | 必须按模板格式输出，否则后处理会失败 |
| "text 字段我翻译/总结了一下" | text 必须是原文精确子串，summary_cn 才是总结 |
| "不需要运行 pipeline.py" | 原始提取缺少 location 和 confidence，必须后处理 |
| "手动写 location 字段" | location 由 Source Grounding 算法填充，禁止手写 |
| "置信度都设 1.0" | confidence 由 4 维度算法计算，禁止手动赋值 |

---

## Anti-Rationalization

| 借口 | 反驳 | 正确做法 |
|------|------|----------|
| "文件太小不需要后处理" | 小文件也需要 Source Grounding 定位 | 运行 pipeline |
| "只有 3 个提取项不需要去重" | 去重是自动的，不增加成本 | 运行 pipeline |
| "这是文档不是代码，不需要 location" | 文档也有行号和字符位置 | 运行 pipeline |
| "用户只要 Markdown 报告" | 先 JSON 再转 Markdown，数据可复用 | 先 pipeline 后报告 |
| "pipeline.py 报错了就跳过" | 修复错误而非跳过后处理 | 调试 pipeline |
| "LLM 提取已经很准确了" | 无后处理的提取缺少定位和评分 | 完整流程 |

---

## Common Mistakes

| 错误 | 症状 | 预防 |
|------|------|------|
| text 字段改写原文 | Source Grounding 全部 match_type=none | 确保 text 是原文子串 |
| 忘记指定 source_file | pipeline 无法分组去重 | 每个提取项必须有 source_file |
| 手动写 location | 与原文位置不一致 | 让 Source Grounding 自动填充 |
| 预设配置路径错误 | pipeline 使用默认配置 | 用绝对路径或相对于 scripts/ 的路径 |
| attributes 缺少必填字段 | confidence 评分偏低 | 按 extraction-types.md 的必填属性 |
| 中文 summary_cn 过长 | KG 注入时实体名截断 | summary_cn 控制在 50 字符内 |

---

## 详细参考

| 文件 | 内容 | 何时读取 |
|------|------|---------|
| `references/extraction-types.md` | 6种类型详细定义和识别规则 | 提取前必读 |
| `references/few-shot-templates.md` | 各场景的 Few-shot 示例 | 提取时参考 |
| `references/output-schema.md` | 完整 JSON Schema | 验证输出时 |
| `references/post-processing.md` | 后处理算法详细说明 | 调试管道时 |
